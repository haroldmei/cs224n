{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Prove softmax(x) = softmax(x + c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove softmax(x) = softmax(x + c) <br>\n",
    "Here we do the math with vectors and matrices directly. \n",
    "softmax(x) is a vector with the same dimention as vector x. <br>\n",
    "$ softmax(x) = \\frac {\\exp(x)} {\\sum {\\exp(x)}}$<br>\n",
    "$\\Rightarrow softmax(x + c) = \\frac {\\exp(c)\\exp(x)} {\\exp(c) \\sum {\\exp(x)}}$ <br>\n",
    "$\\Rightarrow softmax(x + c) = softmax(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Gradient of sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x) = \\frac {1} {1 + \\exp(-x)}$ <br>\n",
    "$\\Rightarrow \\sigma(x) = \\frac {\\exp(x)} {1 + \\exp(x)}$ <br>\n",
    "$\\Rightarrow \\sigma(x) = \\exp(x)\\frac {1} {1 + \\exp(x)}$ <br>\n",
    "$\\Rightarrow \\sigma'(x) = (\\exp(x))'\\frac {1} {1 + \\exp(x)} + \\exp(x)(\\frac {1} {1 + \\exp(x)})' $ <br>\n",
    "$\\Rightarrow \\sigma'(x) = \\frac {\\exp(x)} {1 + \\exp(x)} + \\exp(x) \\frac {\\exp(x)} {(1 + \\exp(x))^2}$ <br>\n",
    "$\\Rightarrow \\sigma'(x) = \\frac {\\exp(x)} {1 + \\exp(x)} (1 - \\frac {\\exp(x)} {1 + \\exp(x)})$ <br>\n",
    "$\\Rightarrow \\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Gradient w.r.t softmax and cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: <br>\n",
    "$J = CE(y,\\hat y) = -\\sum y_i \\log(\\hat y_i)$ <br>\n",
    "$\\hat y = softmax(\\theta) $ <br>\n",
    "\n",
    "The cross entropy can simply be the dot product of two vectors: <br>\n",
    "$CE(y, \\hat y) = -\\log(\\hat y)y^T$\n",
    "\n",
    "$\\frac {\\partial CE} {\\partial \\theta} = \\frac {\\partial CE}{\\partial \\hat y} \\frac {\\partial \\hat y}{\\partial \\theta}$ <br>\n",
    "Where: $\\frac {\\partial CE}{\\partial \\hat y} = -\\frac{1}{\\hat y}y^T$ is a scaler. $\\frac{1}{\\hat y}$ is a row vector and $y^T$ is a column vector. <br>\n",
    "and: $\\frac {\\partial \\hat y}{\\partial \\theta} = (\\exp(\\theta) \\frac {1}{\\sum \\exp(\\theta)})'\n",
    "= (\\exp(\\theta))'\\frac {1}{\\sum \\exp(\\theta)} + \\exp(\\theta)(\\frac {1}{\\sum \\exp(\\theta)})'$ <br>\n",
    "$\\Rightarrow \\frac {\\partial \\hat y}{\\partial \\theta} = \\frac {\\exp(\\theta)}{\\sum \\exp(\\theta)} - \\frac {(\\exp(\\theta))^2}{(\\sum \\exp(\\theta))^2}$ <br>\n",
    "$\\Rightarrow \\frac {\\partial \\hat y}{\\partial \\theta} = \\hat y \\circ (1 - \\hat y)$ <br>\n",
    "$\\Rightarrow \\frac {\\partial CE} {\\partial \\theta} = -\\frac{1}{\\hat y}y^T (\\hat y \\circ (1 - \\hat y))$ <br>\n",
    "Symbol $\\circ$ denotes element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Gradient w.r.t one-hidden-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: <br>\n",
    "$J = CE(y,\\hat y) = -\\sum y_i \\log(\\hat y_i) = -\\log(\\hat y_i)y^T$ <br>\n",
    "$\\hat y = softmax(hW_2 + b_2) $ <br>\n",
    "$h = \\sigma(xW_1 + b_1) $ <br>\n",
    "Find $\\frac {\\partial J}{\\partial x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Gradient w.r.t centor word vector in Skip-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: <br>\n",
    "$\\hat y_o = p(o|c) = \\frac{\\exp(u_o^Tv_c)}{\\sum_{w=1}^V \\exp(u_w^Tv_c)}$ <br>\n",
    "$J_{softmax-CE}(o,v_c,U) = CE(y,\\hat y) = -\\log(\\hat y) \\cdot y^T$ <br>\n",
    "Find $\\frac {\\partial J_{softmax-CE}}{\\partial v_c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Gradient w.r.t output word vector in Skip-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Gradient w.r.t center/output word vectors when using negative sampling loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: <br>\n",
    "$\\hat y_o = p(o|c) = \\frac{\\exp(u_o^Tv_c)}{\\sum_{w=1}^V \\exp(u_w^Tv_c)}$ <br>\n",
    "$J_{neg-sample}(o,v_c,U) = -\\log(\\sigma(u_o^T v_c)) - \\sum_{k=1}^K \\log(\\sigma(-u_k^T v_c))$ <br>\n",
    "Find $\\frac {\\partial J_{neg-sample}}{\\partial v_c}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
