{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Prove softmax(x) = softmax(x + c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove softmax(x) = softmax(x + c) <br>\n",
    "Here we do the math with vectors and matrices directly. \n",
    "softmax(x) is a vector with the same dimention as vector x. <br>\n",
    "$$softmax(x) = \\frac {\\exp(x)} {\\sum {\\exp(x)}} \\Rightarrow$$<br>\n",
    "$$softmax(x + c) = \\frac {\\exp(c)\\exp(x)} {\\exp(c) \\sum {\\exp(x)}} \\Rightarrow$$ <br>\n",
    "$$softmax(x + c) = softmax(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Gradient of sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(x) = \\frac {1} {1 + \\exp(-x)}\\Rightarrow $$ <br>\n",
    "$$\\sigma(x) = \\frac {\\exp(x)} {1 + \\exp(x)} \\Rightarrow $$ <br>\n",
    "$$\\sigma(x) = \\exp(x)\\frac {1} {1 + \\exp(x)} \\Rightarrow $$ <br>\n",
    "$$\\sigma'(x) = (\\exp(x))'\\frac {1} {1 + \\exp(x)} + \\exp(x)(\\frac {1} {1 + \\exp(x)})' \\Rightarrow $$ <br>\n",
    "$$\\sigma'(x) = \\frac {\\exp(x)} {1 + \\exp(x)} + \\exp(x) \\frac {\\exp(x)} {(1 + \\exp(x))^2} \\Rightarrow $$ <br>\n",
    "$$\\sigma'(x) = \\frac {\\exp(x)} {1 + \\exp(x)} (1 - \\frac {\\exp(x)} {1 + \\exp(x)}) \\Rightarrow $$ <br>\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Gradient w.r.t softmax and cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: <br>\n",
    "$$J = CE(y,\\hat y) = -\\sum y_i \\log(\\hat y_i)$$\n",
    "$$\\hat y = softmax(\\theta) $$\n",
    "\n",
    "The cross entropy can simply be the dot product of two vectors: <br>\n",
    "$$CE(y, \\hat y) = -\\log(\\hat y)y^T$$\n",
    "\n",
    "$$\\frac {\\partial CE} {\\partial \\theta} = \\frac {\\partial CE}{\\partial \\hat y} \\frac {\\partial \\hat y}{\\partial \\theta}$$ <br>\n",
    "Where: $\\frac {\\partial CE}{\\partial \\hat y} = -\\frac{1}{\\hat y}y^T$ is a scaler. $\\frac{1}{\\hat y}$ is a row vector and $y^T$ is a column vector. <br>\n",
    "and: \n",
    "$$\\frac {\\partial \\hat y}{\\partial \\theta} = (\\exp(\\theta) \\frac {1}{\\sum \\exp(\\theta)})'\n",
    "= (\\exp(\\theta))'\\frac {1}{\\sum \\exp(\\theta)} + \\exp(\\theta)(\\frac {1}{\\sum \\exp(\\theta)})' \\Rightarrow $$<br>\n",
    "$$\\frac {\\partial \\hat y}{\\partial \\theta} = \\frac {\\exp(\\theta)}{\\sum \\exp(\\theta)} - \\frac {(\\exp(\\theta))^2}{(\\sum \\exp(\\theta))^2} \\Rightarrow $$<br>\n",
    "$$\\frac {\\partial \\hat y}{\\partial \\theta} = \\hat y \\circ (1 - \\hat y) \\Rightarrow $$<br>\n",
    "$$\\frac {\\partial CE} {\\partial \\theta} = -\\frac{1}{\\hat y}y^T (\\hat y \\circ (1 - \\hat y))$$<br>\n",
    "Symbol $\\circ$ denotes element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Gradient w.r.t one-hidden-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: <br>\n",
    "$$J = CE(y,\\hat y) = -\\sum y_i \\log(\\hat y_i) = -\\log(\\hat y_i)y^T$$ <br>\n",
    "$$\\hat y = softmax(hW_2 + b_2) $$ <br>\n",
    "$$h = \\sigma(xW_1 + b_1) $$ <br>\n",
    "Find $$\\frac {\\partial J}{\\partial x}$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Gradient w.r.t centor word vector in Skip-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: <br>\n",
    "$$\\hat y_o = p(o|c) = \\frac{\\exp(u_o^Tv_c)}{\\sum_{w=1}^V \\exp(u_w^Tv_c)}$$ <br>\n",
    "$$J_{softmax-CE}(o,v_c,U) = CE(y,\\hat y) = -\\log(\\hat y) \\cdot y^T$$<br>\n",
    "Find $$\\frac {\\partial J_{softmax-CE}}{\\partial v_c}$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose the center and output word vectors are D dimensional column vectors (From the definition of $\\hat y$ we know that word vectors are column vectors). <br>\n",
    "$\\hat y$ is again a vector with the same dimension as the one-hot-vector y which is the ground truth of Skip-Gram model;\n",
    "Which means given a center word $v_c$, the ones that appear to be neighbours in it's window will have '1's in those indexes.\n",
    "Maybe in this case it doens't make sense to call it 'one-hot-vector', but rather 'multi-hot-vector' because there will be multiple indexes that will be '1'.\n",
    "\n",
    "I will try again making vectorized derivation. <br>\n",
    "$$\\hat y = p(\\circ | v_c) = \\frac{\\exp(U^T v_c)}{\\sum \\exp (U^T v_c)}$$ <br>\n",
    "A vector of probabilities of how likely it is a neighbour of the given center word $v_c$; <br>\n",
    "The cross entropy loss again is just a dot product of the ground truth vector and the minus log-likeliehood: <br> <br>\n",
    "$$J = -\\log(\\hat y) y^T $$\n",
    "Again we assume it is a row vector. <br>\n",
    "\n",
    "Chain rule of derivation for the center word: <br>\n",
    "$$\\frac {\\partial J}{\\partial v_c} = \\frac{\\partial J}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial (U^Tv_c)} \\frac{\\partial (U^Tv_c)}{\\partial v_c}$$ <br>\n",
    "$$\\frac {\\partial J}{\\partial v_c} = -\\frac{1}{\\hat y} y^T \\cdot \\hat y \\circ (1 - \\hat y) \\cdot U $$ <br>\n",
    "Dimensions of different terms: <br>\n",
    "$-\\frac{1}{\\hat y} y^T$: a scalar since it is a dot product of two [1 x V] vectors;<br>\n",
    "$\\hat y \\circ (1 - \\hat y)$: Element-wise multiplication, end up with a [1 x V] row vector;<br>\n",
    "$U$: by definition, $U = [u_1, u_2, ..., u_V]$ is just a [V x D] matrix, Dx is the size of the word vector.<br>\n",
    "$\\frac {\\partial J}{\\partial v_c}$ is then a [1 x D] vector, same size as $v_c$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Gradient w.r.t output word vector in Skip-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again consider the chain rule of derivation, we have the last term different from the one in 3.a:<br>\n",
    "$$\\frac {\\partial J}{\\partial u_k} = \\frac{\\partial J}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial (U^Tv_c)} \\frac{\\partial (U^Tv_c)}{\\partial u_k}$$  <br>\n",
    "Since $U = [u_1, u_2, ..., u_V], U^Tv_c = [u_1, u_2, ..., u_V]^Tv_c$, the last term will be just:  <br> <br>\n",
    "$$\\frac{\\partial (U^Tv_c)}{\\partial u_k} = [...,v_c,...]$$  <br>\n",
    "The tricky part here is that here the result of $\\frac {\\partial J}{\\partial u_k}$ is a [V x D] matrix, with the kth row as $v_c$, all others are 0. <br>\n",
    "\n",
    "Another way of doing the math is to take derivative w.r.t. the whole output matrix U, and the last term will simply end up with $v_c$, a [1 x D] vector, left multiply it by \n",
    "$$-\\frac{1}{\\hat y} y^T \\cdot \\hat y \\circ (1 - \\hat y)$$ <br>\n",
    "this time we use outer product. I will come back and explain it a bit later. <br>\n",
    "\n",
    "Note that this just uses for one center word; the final loss is going to be a sum over all words in the vocabulary as center words, and hence the gradient w.r.t. $u_k$ is also going to be a sum over all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Gradient w.r.t center/output word vectors when using negative sampling loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: <br>\n",
    "$$\\hat y_o = p(o|c) = \\frac{\\exp(u_o^Tv_c)}{\\sum_{w=1}^V \\exp(u_w^Tv_c)}$$  <br>\n",
    "$$J_{neg-sample}(o,v_c,U) = -\\log(\\sigma(u_o^T v_c)) - \\sum_{k=1}^K \\log(\\sigma(-u_k^T v_c))$$  <br>\n",
    "Find $$\\frac {\\partial J_{neg-sample}}{\\partial v_c}$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this time the sigmoid function is used instead of softmax, I find it no long a good idea to use matrix U. For both computation efficiency and ease of math.<br>\n",
    "Do the derivation just use $u_o,v_c,u_k$.<br>\n",
    "\n",
    "Derivative w.r.t. $v_c$ will contain two terms for the expected and negative each <br>\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial v_c} = \n",
    "\\frac{\\partial J}{\\partial(-log(\\sigma(u_o^Tv_c)))} \\frac{\\partial(-log(\\sigma(u_o^Tv_c)))}{\\partial u_o^Tv_c} \\frac{\\partial u_o^Tv_c}{\\partial v_c} + \n",
    "\\frac{\\partial J}{\\partial(-\\sum_{k=1}^K log(\\sigma(-u_k^Tv_c)))} \\frac{\\partial(-\\sum_{k=1}^K log(\\sigma(-u_k^Tv_c)))}{\\partial u_o^Tv_c} \\frac{\\partial u_o^Tv_c}{\\partial v_c}$$ <br>\n",
    "\n",
    "$$ = -\\frac{1}{\\sigma(u_o^Tv_c)} \\sigma(u_o^Tv_c)(1 - \\sigma(u_o^Tv_c))u_o + \n",
    "-\\sum_{k=1}^K (\\frac{1}{\\sigma(-u_k^Tv_c)} (-\\sigma(-u_k^Tv_c))(1 - \\sigma(-u_k^Tv_c))u_k)$$\n",
    "\n",
    "\n",
    "Derivative w.r.t. $u_o$ will contain only one term because $o \\notin {1,2,...,K}$ <br> <br>\n",
    "$$\\frac {\\partial J}{\\partial u_o} = -\\frac{1}{\\sigma(u_o^Tv_c)} \\sigma(u_o^Tv_c)(1 - \\sigma(u_o^Tv_c))v_c\n",
    "\\frac {\\partial J}{\\partial u_{i!=o}} = -\\frac{1}{\\sigma(-u_k^Tv_c)} (-\\sigma(-u_k^Tv_c))(1 - \\sigma(-u_k^Tv_c))v_c$$ <br>\n",
    "and 0 for all other output words $u_j$.\n",
    "\n",
    "Negative sampling is much more effective in that it needs no softmax computation, for each output word a softmax needs V vector multiplication while negative sampling only needs K+1 vector multiplication.<br>\n",
    "So the speed-up ratio is up to V/K, which in a lot of cases we may have millions of vocabulary while we can choose a K of only several hundreds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d Gradient of Skip-Gram and CBOW models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram model: <br>\n",
    "$$\\frac{\\partial J}{\\partial v_c} = \\sum_{-m \\le j \\le m, j\\ne0}\\frac{\\partial F(w_{t+j}, v_c)}{\\partial v_c}$$\n",
    "For $o \\in [w_{t-m},...,w_{t-1}, w_{t+1}, ..., w_{t+m}]$:<br>\n",
    "$$\\frac{\\partial J}{\\partial u_o} = \\frac{\\partial F(o, v_c)}{\\partial u_o}$$\n",
    "Otherwise it is 0.\n",
    "\n",
    "CBOW model: <br>\n",
    "For $-m \\le j \\le m$ and $j \\ne 0$: <br>\n",
    "$$\\frac{\\partial J}{\\partial v_{w_{t+j}}} = \\frac{\\partial F(w_t, v_{w_{t+j}})}{\\partial v_{w_{t+j}}}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial u_{w_t}} = \\frac{\\partial F(u_{w_t}, \\hat v)}{\\partial u_{w_t}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
